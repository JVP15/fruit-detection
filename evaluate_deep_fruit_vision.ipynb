{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Fruit Vision Evaluation\n",
    "\n",
    "Our detection, ripeness, and defect models are all trained and tested on different datasets. This notebook evaluates the overall performance of Deep Fruit Vision on a unique, hand-labelled test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from deepfruitvision import DeepFruitVision\n",
    "from modules.datasets import EnsembleDataset, save_dataset\n",
    "from torchvision.ops import box_iou\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_classification_weights = os.path.join('weights', 'detection', 'best_classification.pt')\n",
    "best_detection_weights = os.path.join('weights', 'detection', 'best_detection.pt')\n",
    "ripeness_weights = os.path.join('weights', 'ripeness', 'ripeness_model_fine_tuned')\n",
    "defect_weights = os.path.join('weights', 'defect', 'defect_model_fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use the EnsembleDataset class to easily load images and labels from the test dataset. We reserve a bit of the ensemble dataset for fine-tuning our models. We use the seed to make sure that we get the same images across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_dir = os.path.join('dataset', 'deepfruitvision_eval')\n",
    "detection_eval_img_dir = os.path.join(eval_dir, 'images')\n",
    "detection_eval_label_dir = os.path.join(eval_dir, 'labels')\n",
    "\n",
    "os.makedirs(detection_eval_img_dir, exist_ok=True)\n",
    "os.makedirs(detection_eval_label_dir, exist_ok=True)\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "num_fine_tune_samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ensemble_dataset = EnsembleDataset('dataset', for_yolov5=True)\n",
    "\n",
    "random_indices = np.random.permutation(len(ensemble_dataset))\n",
    "fine_tune_indices = random_indices[:num_fine_tune_samples]\n",
    "test_indices = random_indices[num_fine_tune_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving dataset to dataset\\deepfruitvision_eval\\images and dataset\\deepfruitvision_eval\\labels: 100%|██████████| 105/105 [00:01<00:00, 87.31it/s]\n"
     ]
    }
   ],
   "source": [
    "yolov5_test_dataset = Subset(ensemble_dataset, test_indices)\n",
    "save_dataset(yolov5_test_dataset, detection_eval_img_dir, detection_eval_label_dir) # save the test split of the ensemble dataset with Yolo-v5 labels to the disk\n",
    "\n",
    "ensemble_dataset.for_yolov5 = False\n",
    "deepfruitvision_test_dataset = Subset(ensemble_dataset, test_indices) # then just get the test split of the ensemble dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Detection mAP\n",
    "\n",
    "The detection/Yolo-v5 model is responsible for detecting and classifying fruits, so we can just use Yolo-v5's built-in evaluation script. We've provided two Yolo-v5 models. One is better at detecting fruits (high bounding box accuracy, but has difficulty classifying fruits) and the other is better at classification (better at classifying fruits, but has less accurate bounding boxes). We'll evaluate both models and compare their performances.\n",
    "\n",
    "The following two tests are each Yolo-v5 model on the detection and classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run yolov5/val.py --data fine_tune_apple_papaya_mango.yaml --weights {best_detection_weights} --img 416 --task test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run yolov5/val.py --data fine_tune_apple_papaya_mango.yaml --weights {best_classification_weights} --img 416 --task test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The `best classification` appears to do much better than the 'best detection' weights, but that's because it often mixed up the classes. If we ignore the classes and just judge the models by detection performance, we can see how good the `best detection` model is at fruit detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run yolov5/val.py --data fine_tune_apple_papaya_mango.yaml --weights {best_detection_weights} --img 416 --task test --single-cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%run yolov5/val.py --data fine_tune_apple_papaya_mango.yaml --weights {best_classification_weights} --img 416 --task test --single-cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now we can clean the Yolov5 eval dir because we don't need it any more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# also, clean up the yolov5 eval dataset\n",
    "shutil.rmtree(eval_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ensemble Classification Accuracy\n",
    "\n",
    "Now we want to evaluate the accuracy of the ensemble classification model. Since we only care about the accuracy of the harvestability, we ignore any bounding box that is too small to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5  2022-11-21 Python-3.10.5 torch-1.12.0 CUDA:0 (NVIDIA GeForce GTX 1070, 8192MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "fruit_vision = DeepFruitVision(best_classification_weights, ripeness_weights, defect_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use Matplotlib to display the true and predicted bounding boxes\n",
    "# true bounding boxes are a list of dicts with x, y, w, and h keys (all normalized, not pixel values)\n",
    "# predicted bounding boxes are a list of dicts with xmin, ymin, xmax, ymax (all normalized, not pixel values)\n",
    "def display_bboxes(img, true_bboxes, pred_bboxes):\n",
    "    img_h, img_w, _ = img.shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    for bbox in true_bboxes:\n",
    "        x = bbox['x'] * img_w\n",
    "        y = bbox['y'] * img_h\n",
    "        w = bbox['w'] * img_w\n",
    "        h = bbox['h'] * img_h\n",
    "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='g', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    for bbox in pred_bboxes:\n",
    "        x = bbox['xmin'] * img_w\n",
    "        y = bbox['ymin'] * img_h\n",
    "        w = (bbox['xmax'] - bbox['xmin']) * img_w\n",
    "        h = (bbox['ymax'] - bbox['ymin']) * img_h\n",
    "        rect = matplotlib.patches.Rectangle((x, y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(1):\n",
    "    img, true_boxes = deepfruitvision_test_dataset[i]\n",
    "    pred_boxes = fruit_vision.get_harvestability(img)\n",
    "    display_bboxes(img, true_boxes, pred_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_ripeness(img, true_box):\n",
    "    # get the fruit from the image\n",
    "    img_h, img_w, _ = img.shape\n",
    "    x, y, w, h = int(true_box['x'] * img_w), int(true_box['y'] * img_h), int(true_box['w'] * img_w), int(true_box['h'] * img_h)\n",
    "    cropped_img = img[y:y+h, x:x+w]\n",
    "\n",
    "    # resize the image to 224x224\n",
    "    cropped_img = cv2.resize(cropped_img, (224, 224))\n",
    "    # convert it to a tf tensor and normalize it\n",
    "    cropped_img = tf.convert_to_tensor(cropped_img, dtype=tf.float32)\n",
    "\n",
    "    # add a batch dimension\n",
    "    cropped_img = tf.expand_dims(cropped_img, axis=0)\n",
    "    # feed it to deepfruitvision's ripeness model\n",
    "    ripeness = fruit_vision.ripeness_module.get_ripeness_predictions(cropped_img)\n",
    "    return ripeness[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluating DeepFruitVision:   0%|          | 0/105 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9641d312385a47a6aa434ba721567944"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 264 out of 338 correct (78.11%)\n",
      "Got 320 out of 338 correct bounding boxes (94.67%)\n",
      "Got 274 out of 338 correct ripeness labels (81.07%)\n",
      "Got 278 out of 338 correct ripeness labels when the bounding box was correct (82.25%)\n",
      "Got 292 out of 338 correct defect labels (86.39%)\n"
     ]
    }
   ],
   "source": [
    "min_box_size = fruit_vision.min_bounding_box_size\n",
    "\n",
    "total_boxes = 0\n",
    "total_correct = 0\n",
    "\n",
    "correct_bbox = 0\n",
    "correct_ripeness = 0\n",
    "correct_ripeness_true_bbox = 0\n",
    "correct_defect = 0\n",
    "\n",
    "for frame, true_boxes in tqdm(deepfruitvision_test_dataset, desc='Evaluating DeepFruitVision', total=len(deepfruitvision_test_dataset)):\n",
    "    predicted_boxes = fruit_vision.get_harvestability(frame)\n",
    "\n",
    "    # ignore any boxes in the predicted and true boxes that are too small to be classified\n",
    "    predicted_boxes = [box for box in predicted_boxes if box['xmax'] - box['xmin'] > min_box_size and box['ymax'] - box['ymin'] > min_box_size]\n",
    "    # the true boxes are numpy arrays with the format [class, x, y, w, h, ...]\n",
    "    true_boxes = [box for box in true_boxes if box['w'] > min_box_size and box['h'] > min_box_size]\n",
    "\n",
    "    if len(true_boxes) == 0: # if there are no boxes large enough to be classified, then we can't evaluate this frame\n",
    "        continue\n",
    "\n",
    "    total_boxes += len(true_boxes)\n",
    "\n",
    "    if len(predicted_boxes) == 0: # if there are no predicted boxes, then we automatically get 0 correct\n",
    "        continue\n",
    "    \n",
    "    # now we have to convert the predicted and true boxes to tensors so we can use Yolo-v5's built-in IoU function\n",
    "    predicted_boxes_tensor = torch.tensor([[box['xmin'], box['ymin'], box['xmax'], box['ymax']] for box in predicted_boxes])\n",
    "    # each box has to have the format [x1, y1, x2, y2] where x1 < x2 and y1 < y2\n",
    "    true_boxes_tensor = torch.tensor([[box['x'], box['y'], box['x'] + box['w'], box['y'] + box['h']] for box in true_boxes])\n",
    "\n",
    "    # this returns a tensor [num_true_boxes, num_predicted_boxes] that we can use to determine which of the true boxes have a corresponding predicted box\n",
    "    iou = box_iou(true_boxes_tensor, predicted_boxes_tensor)\n",
    "\n",
    "    # the max of each row will be the IoU of the predicted box with the true box\n",
    "    max_ious, max_iou_indices = torch.max(iou, dim=1)\n",
    "\n",
    "    for i, (max_iou, max_iou_index) in enumerate(zip(max_ious, max_iou_indices)):\n",
    "\n",
    "        ripeness_true_bbox  = get_ripeness(frame, true_boxes[i])\n",
    "\n",
    "        if ripeness_true_bbox[0] == true_boxes[i]['ripeness']:\n",
    "            correct_ripeness_true_bbox += 1\n",
    "\n",
    "        if max_iou > 0.5: # if the IoU is greater than 0.5, then we consider it a correct prediction\n",
    "            correct_bbox += 1\n",
    "\n",
    "            if ripeness_true_bbox == true_boxes[i]['ripeness']:\n",
    "                correct_ripeness_true_bbox += 1\n",
    "\n",
    "            if true_boxes[i]['ripeness'] == predicted_boxes[max_iou_index]['ripeness'][0]:\n",
    "                correct_ripeness += 1\n",
    "\n",
    "            if true_boxes[i]['defect'] == predicted_boxes[max_iou_index]['defect'][0]:\n",
    "                correct_defect += 1\n",
    "\n",
    "            true_ensemble_label = true_boxes[i]['ensemble']\n",
    "            predicted_harvestability_label = predicted_boxes[max_iou_index]['harvestability']\n",
    "\n",
    "            if true_ensemble_label == predicted_harvestability_label:\n",
    "                total_correct += 1\n",
    "\n",
    "print(f'Got {total_correct} out of {total_boxes} correct ({total_correct / total_boxes * 100:.2f}%)')\n",
    "print(f'Got {correct_bbox} out of {total_boxes} correct bounding boxes ({correct_bbox / total_boxes * 100:.2f}%)')\n",
    "print(f'Got {correct_ripeness} out of {total_boxes} correct ripeness labels ({correct_ripeness / total_boxes * 100:.2f}%)')\n",
    "print(f'Got {correct_ripeness_true_bbox} out of {total_boxes} correct ripeness labels when the bounding box was correct ({correct_ripeness_true_bbox / total_boxes * 100:.2f}%)')\n",
    "print(f'Got {correct_defect} out of {total_boxes} correct defect labels ({correct_defect / total_boxes * 100:.2f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}